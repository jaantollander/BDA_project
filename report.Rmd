---
title: Analysing Critic Scores as a Predictor of Video Game Sales
author: 
- Jaan Tollander de Balsch
- Samuel Piirainen
data: ""
output: 
  pdf_document: 
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
bibliography: bibliography.bib
urlcolor: blue
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/harvard-anglia-ruskin-university.csl
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Libraries
Import the required libraries and set Stan options.
```{r, message = FALSE}
library(ggplot2)
library(gridExtra)
library(bayesplot)
library(analogue)
library(tidyr)
library(plyr)
library(dplyr)
library(rstan)
library(loo)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

This project is available in the [`BDA_project`](https://github.com/jaantollander/BDA_project) GitHub repository.


# Dataset
Load the dataset and filter unnecessary values.
```{r}
table <- read.csv(
  file="data/Video_Games_Sales_as_at_22_Dec_2016.csv", 
  header=TRUE, sep=",", fill=TRUE, stringsAsFactors=FALSE)
data <- filter(
  table, Year_of_Release >= 2009, Year_of_Release <= 2016)
data_scored <- drop_na(
  data, Year_of_Release, Platform, Global_Sales, Critic_Score, Rating) %>%
  filter(Rating != "")
```

## Description
The dataset consists of *video games sales data* as of December 2016 scraped from Metacritic by @video_games_sales. In the dataset, each row represents data for a single game. The relevant fields for each row in the dataset are

* `Platform`: The abbreviation of the platform for playing the game.
* `Year_of_Release`: The year that the game was released. It contains games released between the years 1985 and 2016.
* `Global_Sales`: The number of game sales in millions.
* `Critic_Score`: A score given by critics. A number between 0 and 100.
* `Rating`: Age rating of the game.

In this report we are interested in the relationship between video game sales and critic reviews on the games released between the years 2009 - 2016. We also predict future sales with a posterior predictive distribution. We consider the platforms `N64`, `DC`, `GB` and `WS` statistical anomalies and have excluded them from this report, as they have very little to no actual representation in the dataset during the period of years (2009 - 2016) we analyze the data for.

## Global sales, release years, platforms and critic scores
```{r, echo = FALSE}
# This plot visualizes the total global sales per year.
d1 <- group_by(data, Year_of_Release) %>%
  summarize(sales = sum(Global_Sales))
s1 <- group_by(data_scored, Year_of_Release) %>%
  summarize(sales = sum(Global_Sales))
g1 <- ggplot() + 
  geom_col(aes(x = Year_of_Release, y = sales), d1) +
  geom_col(aes(x = Year_of_Release, y = sales), s1, fill = "red") +
  labs(x = "Year of release", y = "Total global sales")

# This plot visualizes the total global sales per platform.
d2 <- group_by(data, Platform) %>%
  summarize(sales = sum(Global_Sales))
s2 <- group_by(data_scored, Platform) %>%
  summarize(sales = sum(Global_Sales))
g2 <- ggplot() +
  geom_col(aes(x = reorder(Platform, -sales), y = sales), d2) +
  geom_col(aes(x = reorder(Platform, -sales), y = sales), s2, fill = "red") +
  labs(x = "Platform", y = "Total global sales")

# This plot visualizes the total number of games per platform.
d3 <- count(data, Platform)
s3 <- count(data_scored, Platform)
g3 <- ggplot() + 
  geom_col(aes(x = reorder(Platform, -n), y = n), d3) +
  geom_col(aes(x = reorder(Platform, -n), y = n), s3, fill = "red") +
  labs(x = "Platform", y = "Total number of games") +
  theme(legend.position = "right")

#This histogram visualizes counts of the critic scores.
df <- data.frame(critic_score=data_scored$Critic_Score)
g4 <- ggplot(df, aes(x=critic_score)) + geom_histogram(binwidth=2) + xlab("critic scores")
```

```{r}
grid.arrange(g2, g3, ncol = 1)
```

In both plots, the gray color indicates all games and the red color indicates games that are scored by a critic. The data indicates that the critics have rated a large percentage of games by total sales, but a smaller percentage by the total number of games. Therefore, critics have not rated many unpopular games.

```{r}
grid.arrange(g1, g4, ncol = 2)
```

As can be seen from the distribution of the critic scores, it is heavily biased towards the right side, that is, the higher scores. Therefore, the critics seem to be biased towards only rating relatively good games.


## Ratings
```{r, echo=FALSE}
# This histogram visualizes the game age ratings in the dataset.
ratings = c("", "E", "EC", "E10+", "T", "M", "RP")
g5 <- ggplot() + 
  geom_bar(aes(x = factor(Rating, ratings)), data = data) +
  geom_bar(aes(x = factor(Rating, ratings)), data = data_scored, fill = "red") +
  labs(x = "Rating", y = "Total number of games") +
  theme(legend.position = "right")
```

```{r}
g5
```

We can interpret the game age rating as follows. [@ESRBRatings]

* `E` -- Everyone, all ages
* `EC` -- Early childhood, ages 3 and up
* `E10+` -- Everyone 10 and older
* `T` -- Teen, ages 13 and up
* `M` -- Mature, ages 17 and up
* `AO` -- Adults only
* `RP` -- Rating pending, rating has not yet been assigned

There are also games with no rating.


# Analysis problem
Our analysis problem is the question of whether critic scores predict global sales of video games. The critic scores and the sales numbers are assumed to be independent, which means that we assume that the critics have scored the games without the knowledge of their sales. In reality, this assumption might not always be valid. Even if the critics score the game before the publisher has sold any copies, the knowledge about the game's predecessor or the publisher could bias the critic's score.


# Description of the models
We use three models in this report; a separate model, a pooled model, and a hierarchical model. In the separate model, we represent each platform by a separate model with a standard deviation of $\sigma_j$. In the pooled model, we treat all platforms as equal; the whole dataset shares a single mean $\mu$ and standard deviation $\sigma$. In the hierarchical model, we formulate a model where each group represents a single video game platform and uses a shared standard deviation $\sigma$ between all the platforms. Each model computes a fit for both video game sales and critic scores.

```{r}
data_models = filter(
  data_scored, 
  Platform != "N64", Platform != "DC", Platform != "GB", Platform != "WS") %>%
  group_by(Platform, Name, Year_of_Release, Global_Sales, Critic_Score, Rating) %>%
  summarise()
```

Assign integer labels for each platform. Required for the Stan models.
```{r}
platforms = unique(data_models$Platform)
integer_labels = c()
for (i in 1:length(platforms)) {
  integer_labels[platforms[i]] <- i
}
group_indicators <- as.numeric(revalue(x=data_models$Platform, integer_labels))
```

```{r}
integer_labels
```

## Stan model prior choices
We give the scores an adjustable prior because it has a scale that is common to each platform, that is, a value between 0 and 100. We chose an informative prior with a mean of $70$ for the scores, which is reasonably close to the sample mean for `Critic_Score` of `r mean(data_models$Critic_Score)`. This is visible in the histogram above. We set the prior standard deviation to $0.2$ to represent a slight amount of ignorance. We are saying that our best prior guess for the `Critic_Score` mean could be off by a factor of $\exp(0.2)=1.2$. [@Gelman2016]

### Separate model
```{r, warning=FALSE}
data_separate <- list(
  K=length(unique(data_models$Platform)),
  sales=data_models$Global_Sales,
  scores=data_models$Critic_Score,
  N=nrow(data_models),
  x=group_indicators,
  pmu_scores=70,
  psigma_scores=0.2
)

separate_fit <- rstan::stan(
  file = "models/separate.stan", data=data_separate, 
  iter=1000, chains=4, refresh=0)
```

### Hierarchical model
```{r, warning=FALSE}
data_hierarchical <- list(
  K=length(unique(data_models$Platform)),
  sales=data_models$Global_Sales,
  scores=data_models$Critic_Score,
  N=nrow(data_models),
  x=group_indicators,
  pmu_scores=70,
  psigma_scores=0.2
)

hierarchical_fit <- rstan::stan(
  file = "models/hierarchical.stan", data=data_hierarchical, 
  iter=1000, chains=4, refresh=0)
```

### Pooled model
```{r, warning=FALSE}
data_pooled <- list(
  sales=data_models$Global_Sales,
  scores=data_models$Critic_Score,
  N=nrow(data_models),
  pmu_scores=70,
  psigma_scores=0.2
)

pooled_fit <- rstan::stan(
  file = "models/pooled.stan", data=data_pooled, 
  iter=1000, chains=4, refresh=0)
```

## Convergence analysis

We use the potential scale reduction factor $\hat{R}$ for our Stan modelâ€™s convergence analysis. We use the improved version proposed by @Vehtari2019.

```{r, message=FALSE}
grid.arrange(
  mcmc_rhat_hist(rhat(separate_fit)),
  mcmc_rhat_hist(rhat(hierarchical_fit)),
  mcmc_rhat_hist(rhat(pooled_fit))
)
```

The Rstan `Rhat` documentation suggest one to use a sample if the $\hat{R}$ value is less than 1.05. Each model's  $\hat{R}$ satisfies this constraint. We can therefore assume that each model's chains have reached approximate convergence.

## Model selection
We use the PSIS-LOO -method to assess the predictive performance of the pooled, separate and hierarchical Gaussian models in the video game sales dataset. Computations are performed on the R PSIS-LOO implementation, package `loo`.

### Separate LOO

```{r, warning=FALSE}
separate_log_lik_sales <- extract_log_lik(
  separate_fit, parameter_name="sales_log_lik")
separate_rel_n_eff_sales <- relative_eff(
  exp(separate_log_lik_sales), chain_id=rep(1:4, each=500))
separate_loo_sales <- loo(
  separate_log_lik_sales, r_eff=separate_rel_n_eff_sales)
separate_log_lik_scores <- extract_log_lik(
  separate_fit, parameter_name="scores_log_lik")
separate_rel_n_eff_scores <- relative_eff(
  exp(separate_log_lik_scores), chain_id=rep(1:4, each=500))
separate_loo_scores <- loo(
  separate_log_lik_scores, r_eff=separate_rel_n_eff_scores)
```

### Pooled LOO

```{r, warning=FALSE}
pooled_log_lik_sales <- extract_log_lik(
  pooled_fit, parameter_name="sales_log_lik")
pooled_rel_n_eff_sales <- relative_eff(
  exp(pooled_log_lik_sales), chain_id=rep(1:4, each=500))
pooled_loo_sales <- loo(
  pooled_log_lik_sales, r_eff=pooled_rel_n_eff_sales)
pooled_log_lik_scores <- extract_log_lik(
  pooled_fit, parameter_name="scores_log_lik")
pooled_rel_n_eff_scores <- relative_eff(
  exp(pooled_log_lik_scores), chain_id=rep(1:4, each=500))
pooled_loo_scores <- loo(
  pooled_log_lik_scores, r_eff=pooled_rel_n_eff_scores)
```

### Hierarchical LOO

```{r, warning=FALSE}
hierarchical_log_lik_sales <- extract_log_lik(
  hierarchical_fit, parameter_name="sales_log_lik")
hierarchical_rel_n_eff_sales <- relative_eff(
  exp(hierarchical_log_lik_sales), chain_id=rep(1:4, each=500))
hierarchical_loo_sales <- loo(
  hierarchical_log_lik_sales, r_eff=hierarchical_rel_n_eff_sales)
hierarchical_log_lik_scores <- extract_log_lik(
  hierarchical_fit, parameter_name="scores_log_lik")
hierarchical_rel_n_eff_scores <- relative_eff(
  exp(hierarchical_log_lik_scores), chain_id=rep(1:4, each=500))
hierarchical_loo_scores <- loo(
  hierarchical_log_lik_scores, r_eff=hierarchical_rel_n_eff_scores)
```

### Comparison
```{r}
compare_sales <- loo_compare(
  x=list("Pooled" = pooled_loo_sales, 
         "Separate" = separate_loo_sales, 
         "Hierarchical" = hierarchical_loo_sales))
compare_scores <- loo_compare(
  x=list("Pooled" = pooled_loo_scores, 
         "Separate" = separate_loo_scores, 
         "Hierarchical" = hierarchical_loo_scores))
compare_sales
compare_scores
```

We see that the separate model has the largest `elpd` by a significant difference to the pooled and hierarchical models in regards to both of our variables, sales, and scores. Thus the PSIS-LOO method strongly suggests that the separate model produces the most reliable results. Unreliable results for pooled and hierarchical models are likely caused by too much variance between different gaming platforms in terms of sales and scoring. Next, we will continue by analyzing the separate model.

## Separate model posterior visualization

We extract posterior draws of sales and scores for different platforms, and then compute the ratio of sales to scores, and visualize the results. We define the ratio as
$$
r = \frac{\mu_{sales}}{\hat{\mu}_{score}},
$$
where $\hat{\mu}=\mu_{score}/100$ is the normalized score.  We can interpret the ratio $r$ such that when $r$ is lower than the mean of ratios, the critics have overvalued the game, and when it is higher than the mean, the critics have undervalued the game.  A lower variance of ratios indicates that critic scores are better at predicting sales, and higher variance indicates that they are worse at predicting sales.

```{r, echo = FALSE}
separate_values <- extract(separate_fit)
mu_sales <- separate_values$mu_sales
mu_scores <- separate_values$mu_scores / 100

plot_posterior_for_platform <- function(label) {
  i <- integer_labels[label]
  ratio <- mu_sales[,i] / mu_scores[,i]
  df <- data.frame(ratio=ratio)
  ggplot(df, aes(x=as.numeric(rownames(df)), y=ratio)) + 
    geom_line() + 
    xlab(label)
}
```

```{r, echo = FALSE}
p1 <- plot_posterior_for_platform("PC")
p2 <- plot_posterior_for_platform("PS3")
p3 <- plot_posterior_for_platform("Wii")
p4 <- plot_posterior_for_platform("3DS")
```

Plot posterior draw ratio $r$ for platform.
```{r}
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

Plot means and variances for all platforms.
```{r, echo = FALSE}
n = length(platforms)
means_all <- rep(NULL, n)
variances_all <- rep(NULL, n)
for (i in 1:n) {
  ratios <- mu_sales[,i] / mu_scores[,i]
  variances_all[i] <- var(ratios)
  means_all[i] <- mean(ratios)
}

df <- data.frame(platforms=platforms, variances=variances_all, means = means_all)
df.mean_mean = df %>% 
  group_by(platforms) %>% 
  mutate(ymean = mean(means_all))
p5 <- ggplot(df, aes(x=platforms, y=means)) + geom_col() + 
  geom_errorbar(data=df.mean_mean, aes(platforms, ymax = ymean, ymin = ymean, colour="pooled sample mean"), size=0.5, linetype = "longdash", inherit.aes = F, width = 1)
p6 <- ggplot(df, aes(x=platforms, y=variances)) + geom_col()
```

```{r}
grid.arrange(p5, p6, nrow = 2)
```

We can see that different platforms have more variance when it comes to the relationship between sales and critic scores. In particular, the platforms developed by Nintendo (3DS, DS, Wii, WiiU) have more variance, which could indicate that the consumers for these platforms care less about critic opinions. This finding could be explained due to their age ratings since platforms like Wii and DS have a lot more games made for people of all ages in comparison to a platform like the PS3, which we demonstrate in the histograms below.

The sample mean line visualizes how, for certain platforms, critics on average undervalue games (PS3, PS4, Wii, X360) and for others, overvalue games. In other words, for the former platforms, there are more sales despite lower scores on average. The latter (DS, PC, PS2, PSP, PSV) earn higher scores in spite of lower sales. It is important to note that this does not take the amount of console owners into account. One could normalize the sales by the amount of owners for each platform (video game console), but our dataset does not offer this information.

```{r, echo=FALSE}
# Ratings in increasing order
ratings = c("E", "E10+", "T", "M")

plot_rating <- function(platform) {
  df <- filter(data_scored, Platform == platform)
  ggplot() + 
    geom_bar(aes(x=factor(Rating, ratings)), data = df) +
    xlab(platform)  
}
```

```{r, echo=FALSE}
q1 = plot_rating("3DS")
q2 = plot_rating("Wii")
q3 = plot_rating("PS3")
q4 = plot_rating("PC")
```

```{r}
grid.arrange(q1, q2, q3, q4, ncol=3)
```

The barplots are arranged as such that we start from the overall youngest (most games for all ages) playerbase and move into the most adult-oriented platform. The more adult-oriented a platform is, the less variance is has in the barplot above.

## Posterior predictive checking

The stan model computes a posterior predictive distribution for both sales and scores and samples predictions from it. We compute the sample mean (sample size of 2000) for each platform and plot it. We check the posterior predictive capability by comparing it to the observed data.

```{r}
sales_pred <- extract(separate_fit)$sales_pred
scores_pred <- extract(separate_fit)$scores_pred

n <- length(platforms)
sales_pred_mu <- rep(NULL, n)
scores_pred_mu <- rep(NULL, n)

for (i in 1:n) {
  sales_pred_mu[i] <- mean(sales_pred[,i])
  scores_pred_mu[i] <- mean(scores_pred[,i])
}
```

```{r}
df.sales <- data.frame(platforms=platforms, preds=sales_pred_mu)
df.scores <- data.frame(platforms=platforms, preds=scores_pred_mu)

# Sales plotting
sales_preds_plot <- ggplot(df.sales, aes(x=df.sales$platforms, y=df.sales$preds)) + 
  geom_col() + 
  xlab("Predicted") + 
  ylab("Mean sales")
observed_mean_sales_by_platform <- group_by(data, Platform) %>% summarize(sales = mean(Global_Sales))
sales_observed_plot <- ggplot() + 
  geom_col(aes(x = platforms, y = sales), observed_mean_sales_by_platform) + 
  xlab("Observed") + 
  ylab("Mean sales")

# Scores plotting
scores_preds_plot <- ggplot(df.scores, aes(x=df.scores$platforms, y=df.scores$preds)) + 
  geom_col() +
  xlab("Predicted") + 
  ylab("Mean scores") + ylim(0, 100)
observed_mean_scores_by_platform <- group_by(data_scored, Platform) %>% summarize(scores = mean(Critic_Score))
scores_observed_plot <- ggplot() + 
  geom_col(aes(x = platforms, y = scores), observed_mean_scores_by_platform) + 
  xlab("Observed") + 
  ylab("Mean scores") + ylim(0, 100)

grid.arrange(sales_preds_plot, sales_observed_plot)
grid.arrange(scores_preds_plot, scores_observed_plot)
```

The posterior predictive distribution follows the observed data very well for both sales and scores.

We compare the fitted posterior distribution to the observed data in the dataset via the ratio $r$ mean.

```{r}
observed_data <- filter(
  data_scored, 
  Platform != "N64", Platform != "DC", Platform != "GB", Platform != "WS") %>%
  group_by(Platform) %>%
  dplyr::summarize(Ratio_Mean=mean(Global_Sales / (Critic_Score / 100)))
```

```{r}
n <- length(platforms)
means_observed <- rep(NULL, n)
for (i in 1:n) {
  means_observed[i] <- observed_data[i,2]
}
df_observed <- data.frame(platforms=platforms, means_observed=means_observed)
p5_observed <- ggplot(df_observed, aes(x=platforms, y=means_observed)) + geom_col()
```

```{r}
grid.arrange(
  p5 + xlab("Fitted"), 
  p5_observed + xlab("Dataset"), 
  ncol = 1)
```

The fitted variance for the sales to critic scores ratio $r$ follows the dataset variance for the most part. The platforms `Wii` and `X360` are the biggest differences. This can be visualized by dropping them from the barplots:

```{r}
means_observed_drop <- means_observed[-9][-10]
means_fitted_drop <- means[-9][-10]
df_observed_drop <- data.frame(
  platforms=platforms[-9][-10],
  means_observed_drop=means_observed_drop)
p5_observed_drop <- ggplot(
  df_observed_drop, 
  aes(x=platforms, y=means_observed_drop)) + geom_col()

df_fitted_drop <- data.frame(
  platforms=platforms[-9][-10], 
  means_fitted_drop=means_fitted_drop)
p5_fitted_drop <- ggplot(
  df_fitted_drop, 
  aes(x=platforms, y=means_fitted_drop)) + geom_col()
grid.arrange(
  p5_fitted_drop + xlab("Fitted"), 
  p5_observed_drop + xlab("Dataset"), 
  ncol = 1)
```

## Sensitivity analysis for prior choices

We used an informative prior for the critic scores. Let's compare its performance to Stan's default (weakly informative) uniform prior and an altered informative prior with a mean of $50$ and a standard deviation of $0.15$.

```{r, warning=FALSE}
data_separate_uniform <- list(
  K=length(unique(data_models$Platform)),
  sales=data_models$Global_Sales,
  scores=data_models$Critic_Score,
  N=nrow(data_models),
  x=group_indicators
)

separate_fit_uniform <- rstan::stan(
  file = "models/separate-uniform.stan", data=data_separate_uniform, 
  iter=1000, chains=4, refresh=0)
```

```{r, warning=FALSE}
data_separate_altered <- list(
  K=length(unique(data_models$Platform)),
  sales=data_models$Global_Sales,
  scores=data_models$Critic_Score,
  N=nrow(data_models),
  x=group_indicators,
  pmu_scores=50,
  psigma_scores=0.15
)

separate_fit_altered <- rstan::stan(
  file = "models/separate.stan", data=data_separate_altered, 
  iter=1000, chains=4, refresh=0)
```

```{r, echo = FALSE}
separate_values_unif <- extract(separate_fit_uniform)
mu_sales_unif <- separate_values_unif$mu_sales
mu_scores_unif <- separate_values_unif$mu_scores / 100
```

```{r, echo = FALSE}
separate_values_alt <- extract(separate_fit_altered)
mu_sales_alt <- separate_values_alt$mu_sales
mu_scores_alt <- separate_values_alt$mu_scores / 100
```

```{r, echo = FALSE}
n = length(platforms)
means_unif <- rep(NULL, n)
for (i in 1:n) {
  ratios_unif <- mu_sales_unif[,i] / mu_scores_unif[,i]
  means_unif[i] <- mean(ratios_unif)
}
df <- data.frame(platforms=platforms, means=means_unif)
p5_unif <- ggplot(df, aes(x=platforms, y=means)) + geom_col() + xlab("uniform")
```

```{r, echo = FALSE}
n = length(platforms)
means_alt <- rep(NULL, n)
for (i in 1:n) {
  ratios_alt <- mu_sales_alt[,i] / mu_scores_alt[,i]
  means_alt[i] <- mean(ratios_alt)
}
df <- data.frame(platforms=platforms, means=means_alt)
p5_alt <- ggplot(df, aes(x=platforms, y=means)) + geom_col() + xlab("mean 50 std 0.15")
```

```{r}
p5_chosen <- p5 + xlab("mean 70 std 0.2")
grid.arrange(p5_chosen, p5_unif, p5_alt, ncol = 1)
```

Changing the prior for the scores seems to have little effect. Therefore we can say that the prior is not very sensitive to changes and that the data converges to a similar posterior distribution regardless of our choices.

## Discussion of problems and potential improvements

As proven in the previous sections, the model performs very well according to convergence analytics and posterior predictive checking. The model, as it is, currently has one major problem: it does not take the amount of console owners for each platform into account. It assumes each platform to be identical in terms of potential sales. With the knowledge of each platform's consumerbase's size we could likely make the hierarchical model more viable by normalizing the sales-distributions, as they were the separating difference between each platform and led us to use the separate model. Most importantly, this would make the ratio $r$ a more reliable measure of critic overrating particular games.

# References
